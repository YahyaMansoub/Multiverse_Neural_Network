{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0269146a",
   "metadata": {},
   "source": [
    "# Linear Regression from First Principles\n",
    "## A Complete Mathematical and Computational Exploration\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup & Imports](#setup)\n",
    "2. [Mathematical Theory](#theory)\n",
    "   - 1-Neuron Linear Model\n",
    "   - Gradient Derivation for w\n",
    "   - Gradient Derivation for b\n",
    "   - Why Gradient Descent Works\n",
    "   - Backpropagation View\n",
    "3. [Detailed Proofs](#proofs)\n",
    "   - Component-wise Gradient Proof\n",
    "   - 2D Example\n",
    "4. [Implementation](#implementation)\n",
    "   - Basic Functions\n",
    "   - Gradient Computation\n",
    "   - Training Loop\n",
    "5. [Experiment](#experiment)\n",
    "   - Data Generation\n",
    "   - Model Training\n",
    "   - Results Visualization\n",
    "6. [Verification](#verification)\n",
    "7. [Summary](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ad0df",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379dce9b",
   "metadata": {},
   "source": [
    "<a id='theory'></a>\n",
    "## 2. Mathematical Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533444d",
   "metadata": {},
   "source": [
    "# Linear Regression: Mathematical Derivation\n",
    "\n",
    "## 1-Neuron Linear Model\n",
    "\n",
    "We have:\n",
    "* Data: $x_i \\in \\mathbb{R}^d$, target $y_i \\in \\mathbb{R}$\n",
    "* Parameters: $w \\in \\mathbb{R}^d$, $b \\in \\mathbb{R}$\n",
    "* Prediction:\n",
    "$$\\hat{y}_i = w^\\top x_i + b$$\n",
    "\n",
    "**Loss (MSE over $n$ samples):**\n",
    "$$L(w,b) = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_i - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(w^\\top x_i + b - y_i)^2$$\n",
    "\n",
    "Define the error:\n",
    "$$e_i = \\hat{y}_i - y_i = w^\\top x_i + b - y_i$$\n",
    "\n",
    "So:\n",
    "$$L = \\frac{1}{n}\\sum_{i=1}^{n} e_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2afd72",
   "metadata": {},
   "source": [
    "## Gradient w.r.t. $w$\n",
    "\n",
    "Use chain rule per-sample. For one term $e_i^2$:\n",
    "$$\\frac{\\partial}{\\partial w}(e_i^2) = 2e_i\\frac{\\partial e_i}{\\partial w}$$\n",
    "\n",
    "But:\n",
    "$$e_i = w^\\top x_i + b - y_i \\Rightarrow \\frac{\\partial e_i}{\\partial w} = x_i$$\n",
    "\n",
    "So:\n",
    "$$\\frac{\\partial}{\\partial w}(e_i^2) = 2e_i x_i$$\n",
    "\n",
    "Sum and scale by $\\frac{1}{n}$:\n",
    "$$\\nabla_w L = \\frac{1}{n}\\sum_{i=1}^n 2e_i x_i = \\frac{2}{n}\\sum_{i=1}^n (w^\\top x_i + b - y_i)x_i$$\n",
    "\n",
    "**Vectorized form**: Stack rows of $X\\in\\mathbb{R}^{n\\times d}$, $y,\\hat{y}\\in\\mathbb{R}^{n\\times 1}$.\n",
    "\n",
    "Let $e=\\hat{y}-y$. Then:\n",
    "$$\\nabla_w L = \\frac{2}{n}X^\\top e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8b767",
   "metadata": {},
   "source": [
    "## Gradient w.r.t. $b$\n",
    "\n",
    "Again:\n",
    "$$\\frac{\\partial}{\\partial b}(e_i^2) = 2e_i\\frac{\\partial e_i}{\\partial b}$$\n",
    "\n",
    "And:\n",
    "$$\\frac{\\partial e_i}{\\partial b} = 1$$\n",
    "\n",
    "So:\n",
    "$$\\frac{\\partial}{\\partial b}(e_i^2) = 2e_i$$\n",
    "\n",
    "Sum and scale:\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{2}{n}\\sum_{i=1}^n e_i = \\frac{2}{n}\\sum_{i=1}^n (\\hat{y}_i - y_i)$$\n",
    "\n",
    "Vectorized:\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{2}{n}\\mathbf{1}^\\top e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f6769",
   "metadata": {},
   "source": [
    "## Why Gradient Descent Update Works\n",
    "\n",
    "We move opposite the gradient:\n",
    "$$w \\leftarrow w - \\eta \\nabla_w L, \\quad b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "because $\\nabla L$ points in the steepest increase direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8668a",
   "metadata": {},
   "source": [
    "## \"Neural Net\" View (Backprop Lens)\n",
    "\n",
    "This is backprop in 2 local steps:\n",
    "\n",
    "1. $L = \\frac{1}{n}\\sum e^2$\n",
    "   $$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{2}{n}(\\hat{y} - y)$$\n",
    "\n",
    "2. $\\hat{y} = Xw + b$\n",
    "   $$\\frac{\\partial \\hat{y}}{\\partial w} = X, \\quad \\frac{\\partial \\hat{y}}{\\partial b} = 1$$\n",
    "   \n",
    "Chain:\n",
    "$$\\nabla_w L = X^\\top \\frac{\\partial L}{\\partial \\hat{y}}, \\quad \\frac{\\partial L}{\\partial b} = \\sum \\frac{\\partial L}{\\partial \\hat{y}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29670ca2",
   "metadata": {},
   "source": [
    "<a id='proofs'></a>\n",
    "## 3. Detailed Proofs\n",
    "\n",
    "### Why $\\frac{\\partial e_i}{\\partial w} = x_i$?\n",
    "\n",
    "Because $e_i$ is **linear in $w$**.\n",
    "\n",
    "Let $w=(w_1,\\dots,w_d)$, $x_i=(x_{i1},\\dots,x_{id})$. Then:\n",
    "\n",
    "$$e_i = w^\\top x_i + b - y_i = \\sum_{j=1}^d w_j x_{ij} + b - y_i$$\n",
    "\n",
    "Now take **partial derivative wrt one component** $w_k$:\n",
    "\n",
    "$$\\frac{\\partial e_i}{\\partial w_k} = \\frac{\\partial}{\\partial w_k}\\Big(\\sum_{j=1}^d w_j x_{ij} + b - y_i\\Big)$$\n",
    "\n",
    "All terms with $j\\neq k$ are constants wrt $w_k$, so vanish. Only $w_k x_{ik}$ remains:\n",
    "\n",
    "$$\\frac{\\partial e_i}{\\partial w_k} = x_{ik}$$\n",
    "\n",
    "So the gradient wrt the whole vector $w$ is the vector of these partials:\n",
    "\n",
    "$$\\nabla_w e_i = \\begin{bmatrix} \\frac{\\partial e_i}{\\partial w_1} \\\\ \\vdots \\\\ \\frac{\\partial e_i}{\\partial w_d} \\end{bmatrix} = \\begin{bmatrix} x_{i1} \\\\ \\vdots \\\\ x_{id} \\end{bmatrix} = x_i$$\n",
    "\n",
    "Then chain rule:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w}(e_i^2) = 2 e_i \\frac{\\partial e_i}{\\partial w} = 2 e_i x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a2635",
   "metadata": {},
   "source": [
    "### Tiny 2D Example (By Hand)\n",
    "\n",
    "$$e = w_1 x_1 + w_2 x_2 + b - y$$\n",
    "\n",
    "$$\\frac{\\partial e}{\\partial w_1} = x_1, \\quad \\frac{\\partial e}{\\partial w_2} = x_2$$\n",
    "\n",
    "$$\\Rightarrow \\nabla_w e = (x_1, x_2) = x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56d376",
   "metadata": {},
   "source": [
    "<a id='implementation'></a>\n",
    "## 4. Implementation\n",
    "\n",
    "### Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, w, b):\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_hat, y):\n",
    "    return np.mean((y_hat - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2644f34",
   "metadata": {},
   "source": [
    "### Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, y_hat):\n",
    "    n = X.shape[0]\n",
    "    error = y_hat - y\n",
    "    \n",
    "    dw = (2/n) * X.T @ error\n",
    "    db = (2/n) * np.sum(error)\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95b5e5",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cde39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, learning_rate=0.01, epochs=1000):\n",
    "    n, d = X.shape\n",
    "    w = np.random.randn(d)\n",
    "    b = 0.0\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y_hat = forward(X, w, b)\n",
    "        loss = mse(y_hat, y)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        dw, db = compute_gradients(X, y, y_hat)\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f9163",
   "metadata": {},
   "source": [
    "<a id='experiment'></a>\n",
    "## 5. Experiment\n",
    "\n",
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ebd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = np.array([3.0, 2.0])\n",
    "true_b = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X @ true_w + true_b + 0.5 * np.random.randn(n_samples)\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"True parameters: w={true_w}, b={true_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a7ebaf",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_learned, b_learned, losses = train(X, y, learning_rate=0.1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLearned: w={w_learned}, b={b_learned:.4f}\")\n",
    "print(f\"True: w={true_w}, b={true_b}\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744a091",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c67c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c77ffe",
   "metadata": {},
   "source": [
    "<a id='verification'></a>\n",
    "## 6. Verification\n",
    "\n",
    "Let's verify our gradient formulas manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46aa8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = X[:5]\n",
    "y_small = y[:5]\n",
    "\n",
    "w_test = np.array([1.0, 1.0])\n",
    "b_test = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = forward(X_small, w_test, b_test)\n",
    "error = y_hat_test - y_small\n",
    "\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ece4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_small.shape[0]\n",
    "dw_manual = (2/n) * X_small.T @ error\n",
    "\n",
    "print(\"Manual gradient dw:\", dw_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cf7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_func, db_func = compute_gradients(X_small, y_small, y_hat_test)\n",
    "\n",
    "print(\"Function gradient dw:\", dw_func)\n",
    "print(\"Match:\", np.allclose(dw_manual, dw_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb256d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_manual = (2/n) * np.sum(error)\n",
    "\n",
    "print(\"Manual gradient db:\", db_manual)\n",
    "print(\"Function gradient db:\", db_func)\n",
    "print(\"Match:\", np.allclose(db_manual, db_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b8266",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 7. Summary\n",
    "\n",
    "We successfully implemented linear regression from first principles using:\n",
    "\n",
    "1. **Mathematical derivation** of gradients:\n",
    "   - $\\nabla_w L = \\frac{2}{n}X^\\top(Xw + b - y)$\n",
    "   - $\\frac{\\partial L}{\\partial b} = \\frac{2}{n}\\sum(Xw + b - y)$\n",
    "\n",
    "2. **Gradient descent** update:\n",
    "   - $w \\leftarrow w - \\eta \\nabla_w L$\n",
    "   - $b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "3. **Clean, vectorized implementation**\n",
    "\n",
    "### Key Insights:\n",
    "- No activation function needed\n",
    "- Linearity makes gradients straightforward\n",
    "- Error $e_i = \\hat{y}_i - y_i$ appears in both gradients\n",
    "- Matrix operations efficiently compute all derivatives at once"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
